# -*- coding: utf-8 -*-
"""2016 USAJobs Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DsSJroaN9vgZ9KsF4tOd8gJUgFVupvFK
"""

# --- Step 1: Install dotenv (only needed once per runtime)
!pip install python-dotenv

# --- Step 2: Create/load your .env file (this file should NOT go on GitHub!)
from dotenv import load_dotenv
import os


# Load environment variables from .env
load_dotenv()

# --- Step 3: Access your API key safely
api_key = os.getenv("API_KEY")

if not api_key:
    raise ValueError("❌ API key not found! Make sure it's saved in .env")

print("✅ API key loaded successfully (but not printed for safety)")

import requests
import json
import pandas as pd
from copy import deepcopy
import string

host = "data.usajobs.gov"
user_agent = "fizaxkhan04@gmail.com"
url = "https://developer.usajobs.gov/api/HistoricJoa/AnnouncementText"

headers = {
    "Host": host,
    "User-Agent": user_agent,
    "Authorization-Key": api_key
}

response = requests.get(url, headers=headers)
print(response.status_code)
params = {
    "StartPositionOpenDate": "2016-06-05",
    "HiringDepartmentCodes": "AR",
}

response = requests.get(url, headers=headers, params=params)

data = response.json()
pretty_print = json.dumps(data, indent = 4) # making a variable to then print

print(pretty_print)

jobs_2210 = [job for job in data['data'] if any(cat.get("series") == "2210" for cat in job.get("JobCategories", []))]

len(jobs_2210)

titles = []
duties_list = []
qualifications_list = []

for job in jobs_2210:
    title = job.get("title", job.get("announcementNumber", ""))

    duties = job.get("duties", "")
    qualifications = job.get("requirementsQualifications", "")

    titles.append(title)
    duties_list.append(duties)
    qualifications_list.append(qualifications)

# create DataFrame
df_2210 = pd.DataFrame({
    "Job Title": titles,
    "Duties": duties_list,
    "Qualification Summary": qualifications_list
})

display(df_2210.head())

from bs4 import BeautifulSoup

# Apply BeautifulSoup row by row
df_2210["Duties_Clean"] = df_2210["Duties"].apply(
    lambda x: BeautifulSoup(str(x), "html.parser").get_text(separator=" ").strip() if pd.notnull(x) else x
)

# df_2210["Duties_Clean"]
df_2210

df_2210["Qual_Cleaned"] = df_2210["Qualification Summary"].apply(
    lambda x: BeautifulSoup(str(x), "html.parser").get_text(separator=" ").strip() if pd.notnull(x) else x
)

df_2210["Qual_Cleaned"]

# df_2210.drop("Duties", axis=1, inplace=True)
df_2210.drop("Qualification Summary", axis=1, inplace=True)
df_2210.head()

df_2210.rename(columns = {"Duties_Clean": "Duties Clean"}, inplace=True)
df_2210.rename(columns= {"Qual_Cleaned" : "Qualification Summary"}, inplace = True)

df_2210

df_2210["Job Title"] = "IT Specialist"
df_2210

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = stopwords.words('english')

# STAGE 1: turning descriptions to all lowercase
df_2210['lowercase'] = df_2210['Qualification Summary'].apply(lambda x: " ".join(word.lower() for word in x.split()))

df_2210.head()

# STAGE 2: removing punctuation
df_2210['no_punct'] = df_2210['lowercase'].str.replace(f"[{string.punctuation}]", "", regex=True)

df_2210["no_punct"].head()

stop_words = set(stopwords.words('english'))

df_2210['no_stopwords'] = df_2210['no_punct'].apply(lambda x: " ".join(word for word in x.split() if word not in stop_words))

display(df_2210.head())

df_2210['tokens'] = df_2210['no_stopwords'].str.split()

from collections import Counter

all_words = df_2210['tokens'].sum()  # flatten list of lists
word_counts = Counter(all_words)

print(word_counts.most_common(20))  # top 20 words

from nltk.util import ngrams # function for making ngrams
import collections
import re # importing regex package

# need to clean text from 1year/oneyear to 1 year/one year
# \d searching for DIGITS
# \D searching for NON-DIGITS

pattern = r'(\d)([a-zA-Z])'

# insert a space between a digiall_textt and a letter (e.g. "1year" -> "1 year")
cleaned_text = re.sub(pattern, r'\1 \2', " ".join(all_words))

# re-tokenize
tokens = cleaned_text.split()

ngrams_list = ngrams(tokens, 4)
freq = collections.Counter(ngrams_list)

for gram, count in freq.most_common(20):
    if "year" in gram:
        print(gram, count)

import re

# convert letters to numbers

number_words = {
    "one": 1,
    "two": 2,
    "three": 3,
    "four": 4,
    "five": 5,
    "six": 6,
    "seven": 7,
    "eight": 8,
    "nine": 9,
    "ten": 10
}

# Regex to match either a digit or a spelled-out number, then 'year' or 'years'
pattern = r"(\d+|one|two|three|four|five|six|seven|eight|nine|ten)\s+year[s]?"

years_extracted = []

for text in df_2210["Qualification Summary"].dropna():
    matches = re.findall(pattern, text.lower())
    for m in matches:
        # convert spelled-out numbers to digits
        if m.isdigit():
            years_extracted.append(int(m))
        elif m in number_words:
            years_extracted.append(number_words[m])

print("Extracted years:", years_extracted[:20])
print("Total mentions:", len(years_extracted))

freq = Counter(years_extracted)
print(freq.most_common())

# CREATING DF OF YRS OF EXP MENTIONED

# count frequencies
year_counts = Counter(years_extracted)

# create a dataframe
df_years = pd.DataFrame(year_counts.items(), columns=["Years Experience", "Count"])

# sort by Years Experience
df_years = df_years.sort_values("Years Experience").reset_index(drop=True)

df_years

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=df_years)

# SAMPLE CHART

import matplotlib.pyplot as plt

df_years.plot(x="Years Experience", y="Count", kind="bar", legend=False)
plt.xlabel("Required Years of Experience")
plt.ylabel("Number of Mentions")
plt.title("Experience Requirements in 'IT SPECIALIST' Job Postings")
plt.show()