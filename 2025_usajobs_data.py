# -*- coding: utf-8 -*-
"""2025 USAJobs Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xTquGMWr8RsqNKnZJP-TJZOisqtmvk6o
"""

# --- Step 1: Install dotenv (only needed once per runtime)
!pip install python-dotenv

# --- Step 2: Create/load your .env file (this file should NOT go on GitHub!)
from dotenv import load_dotenv
import os

# Load environment variables from .env
load_dotenv()

# --- Step 3: Access your API key safely
api_key = os.getenv("API_KEY")

if not api_key:
    raise ValueError("❌ API key not found! Make sure it's saved in .env")

print("✅ API key loaded successfully (but not printed for safety)")

import requests
import json
import pandas as pd
from copy import deepcopy

host = "data.usajobs.gov"
user_agent = "fizaxkhan04@gmail.com"

url = "https://data.usajobs.gov/api/search"

headers = {
    "Host": host,
    "User-Agent": user_agent,
    "Authorization-Key": api_key
}

response = requests.get(url, headers=headers)
print(response.status_code)

# Parameters for the query
params = {
    "Keyword": "IT Specialist",
    "LocationName": "",
    "ResultsPerPage": 100,
    "Page": 1
}

# Make request
response = requests.get(url, headers=headers, params=params)

data = response.json()
pretty_print = json.dumps(data, indent = 4) # making a variable to then print

print(pretty_print)

qual_sum = data["SearchResult"]["SearchResultItems"][0]["MatchedObjectDescriptor"]["QualificationSummary"]

print(qual_sum)

job_desc = []

for each_job in data["SearchResult"]["SearchResultItems"]:
    qual_summary = each_job["MatchedObjectDescriptor"].get("QualificationSummary", "")
    job_desc.append(qual_summary)

pretty_print = json.dumps(job_desc, indent = 4) # making a variable to then print

print(pretty_print)

jobs = data['SearchResult']['SearchResultItems'] # because jobs is nested in smth else

df = pd.DataFrame(jobs)

df #prints df

def explore_job(job, indent=2):
    """
    Pretty-print one job's data structure so we can inspect where pay grade is stored.
    """
    print(json.dumps(job, indent=indent))

first_job = data["SearchResult"]["SearchResultItems"][1]["MatchedObjectDescriptor"]

explore_job(first_job)

job_title_list = []
qual_summary_list = []

# Assuming data["SearchResult"]["SearchResultItems"] is a list
for job in data["SearchResult"]["SearchResultItems"]:
    descriptor = job.get("MatchedObjectDescriptor", {})

    title = descriptor.get("PositionTitle", "")
    qual_summary = descriptor.get("QualificationSummary", "")

    job_title_list.append(title)
    qual_summary_list.append(qual_summary)

# Create dataframe
usajobs_df = pd.DataFrame({
    "Job Title": job_title_list,
    "Qualification Summary": qual_summary_list,
})

display(usajobs_df)

import nltk

from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words = stopwords.words('english')
len(stop_words)

# STAGE 1: turning descriptions to all lowercase
usajobs_df['lowercase'] = usajobs_df['Qualification Summary'].apply(lambda x: " ".join(word.lower() for word in x.split()))

usajobs_df.head()

# STAGE 2: removing punctuation
import string
usajobs_df['no_punct'] = usajobs_df['lowercase'].str.replace(f"[{string.punctuation}]", "", regex=True)

usajobs_df["no_punct"].head()

stop_words = stopwords.words('english')
len(stop_words)

usajobs_df['no_punct'].apply(lambda x: len([word for word in x.split() if word.lower in stop_words]))

stop_words = set(stopwords.words('english'))

usajobs_df['no_stopwords'] = usajobs_df['no_punct'].apply(lambda x: " ".join(word for word in x.split() if word not in stop_words))

display(usajobs_df.head())

usajobs_df['tokens'] = usajobs_df['no_stopwords'].str.split()

from collections import Counter

all_words = usajobs_df['tokens'].sum()  # flatten list of lists
word_counts = Counter(all_words)

print(word_counts.most_common(20))  # top 20 words

from nltk.util import ngrams # function for making ngrams
import collections
import re # importing regex package

# need to clean text from 1year/oneyear to 1 year/one year
# \d searching for DIGITS
# \D searching for NON-DIGITS

pattern = r'(\d)([a-zA-Z])'

# insert a space between a digiall_textt and a letter (e.g. "1year" -> "1 year")
cleaned_text = re.sub(pattern, r'\1 \2', " ".join(all_words))

# re-tokenize
tokens = cleaned_text.split()

ngrams_list = ngrams(tokens, 4)
freq = collections.Counter(ngrams_list)

for gram, count in freq.most_common(20):
    if "year" in gram:
        print(gram, count)

import re

# convert letters to numbers

number_words = {
    "one": 1,
    "two": 2,
    "three": 3,
    "four": 4,
    "five": 5,
    "six": 6,
    "seven": 7,
    "eight": 8,
    "nine": 9,
    "ten": 10
}

# Regex to match either a digit or a spelled-out number, then 'year' or 'years'
pattern = r"(\d+|one|two|three|four|five|six|seven|eight|nine|ten)\s+year[s]?"

years_extracted = []

for text in usajobs_df["Qualification Summary"].dropna():
    matches = re.findall(pattern, text.lower())
    for m in matches:
        # convert spelled-out numbers to digits
        if m.isdigit():
            years_extracted.append(int(m))
        elif m in number_words:
            years_extracted.append(number_words[m])

print("Extracted years:", years_extracted[:20])
print("Total mentions:", len(years_extracted))

freq = Counter(years_extracted)
print(freq.most_common())

# CREATING DF OF YRS OF EXP MENTIONED

# count frequencies
year_counts = Counter(years_extracted)

# create a dataframe
df_years = pd.DataFrame(year_counts.items(), columns=["Years Experience", "Count"])

# sort by Years Experience
df_years = df_years.sort_values("Years Experience").reset_index(drop=True)

df_years

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=df_years)

# SAMPLE CHART

import matplotlib.pyplot as plt

df_years.plot(x="Years Experience", y="Count", kind="bar", legend=False)
plt.xlabel("Required Years of Experience")
plt.ylabel("Number of Mentions")
plt.title("Experience Requirements in 'IT SPECIALIST' Job Postings")
plt.show()